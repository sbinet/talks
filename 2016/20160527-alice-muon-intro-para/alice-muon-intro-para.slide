Introduction to parallel programming
ALICE Muon Week, 2016/05/27

Sebastien Binet
CNRS/IN2P3/LPC
binet@cern.ch

* Disclaimer

I used to be a `FORTRAN` programmer, then, a `C++` programmer, and after that, a `python` programmer.

Nowadays, I am a [[https://golang.org][Go]] programmer at heart, enjoying blazing fast compilation time, builtin concurrency constructs, easy installation and deployment, etc...

I really learned parallel/concurrent programming with `Go`.
My views on parallel programming with `C++` are thus somewhat skewed.

.image figs/gopher.png
.caption The Go mascott: a gopher

* Parallel programming

Parallel programming is hard.
Parallel programming is harder in `C++`.

Actually, why do we have to do *parallel* programming ?

* Moore's law

.image figs/cpu-free-lunch.png _ 550

* The hardware/software contract

.image figs/par-prog-old-days.png _ 850

* 

.image figs/par-prog-power.png _ 850

* 

.image figs/par-prog-cores.png _ 850

* Free lunch is over

.image figs/head-on.png _ 900

* Raining transistors

- Frequency has plateaued
- *But* number of transistors is still following (more or less) Moore's Law

What can we do with them ?

* Hardware diversity: basic building blocks

.image figs/par-prog-building-blocks.png

* Hardware diversity: combining building blocks

.image figs/par-prog-heterogeneous.png

* Hardware diversity: CPUs

.image figs/par-prog-cpu.png _ 800

* Parallel programming

Ok, so we have to do parallel programming to properly utilize our new
hardware...

Actually, are we really sure we want to do *parallel* programming ?

* Concurrency vs Parallelism

_Concurrency:_ programming as the composition of independently executing processes/tasks.

_Parallelism:_ programming as the simultaneous execution of (possibly related) computations.

.image figs/conc-vs-par.png 350 _

* Interlude: Sequential, Concurrent & Parallel pizzas

* Pizza recipe

(*Disclaimer:* don't ever eat any pizza prepared or cooked by me.)

How to prepare a (sequential) pizza?

.code code/make-pizza.f

Estimated time (1 chef, 1 pizza):

  xx-oooo-xxx-oo-###

How to make this faster?

* (Sequential) Pizza recipe
 
Tasks:

- wash tomatoes and onions
- cut tomatoes, onions
- prepare pizza dough
- put tomato sauce on top of pizza dough
- toppings: put tomatoes, onions, ham and mozarella
- (pre-)heat oven, bake
- (cut, then eat)

Estimated time (1 chef, 1 pizza):

  xx-oooo-xxx-oo-###

* Concurrent pizzas - Parallel pizzas

Estimated time (1 chef, 1 kitchen, 2 pizzas):

  xx-oooo-xxx-oo-###-xx-oooo-xxx-oo-###

Estimated time (1 chef, 2 kitchens, 2 pizzas):

  xx-oooo-xxx-oo+###
                +xx-oooo-xxx-oo-###

Estimated time (2 chefs, 1 kitchen, 2 pizzas):

  xx-xxx-+-xx-xxx-+
         +###     +###
  oooo-oo+-oooo-oo+

Estimated time (2 chefs, 2 kitchens, 2 pizzas):

  xx-oooo-xxx-oo-###
  xx-oooo-xxx-oo-###

* Concurrency vs Parallelism

Concurrency is about dealing with lots of things at once.
Parallelism is about doing lots of things at once.

Concurrency is about (program) *structure*.
Parallelism is about (program) *execution*.

.image figs/conc-vs-par-prog.png 350 _

* Concurrency vs Parallelism

Concurrency and parallelism are related.
Concurrency isn't parallelism (it's better!)

Parallelizing an application is done by:

- finding concurrency in the problem
- exposing the concurrency in the source code
- exploiting the exposed concurrency to complete the job in less time.

.image figs/conc-vs-par-decomp.png

* Decomposition in parallel programs

Every parallel program is based on concurrency
i.e: tasks defined by an application that can run at the same time.

*EVERY* parallel program requires a _task_decomposition_ and a _data_decomposition_:

- Task decomposition: break the application down into a set of tasks that can execute concurrently.
- Data decomposition: How must the data be broken down into chunks and associated with threads/processes to make the parallel program run efficiently.

* Goldilocks

Parallel approaches have to find the "sweet spot" between two extremes.

Too fine grained:

- Data: computation dominated by overhead
- Tasks: context switching overhead

Too coarse grained:

- Data: load balancing problems
- Tasks: insufficient items to keep processes busy

* Parallel programming: multi-process vs multi-threaded

Multi-process:

- resource requirements are multiplied w/ nbr of process instances
- (clever) use of `fork(2)` can mitigate this issue (but not in a `MT` environment)
- one process can not corrupt the memory of another process
- overhead of pushing data from one process to the other

Multi-threaded:

- small context switch times (wrt an OS' process)
- automatic sharing of many hardware resources (memory, fds, sockets...)
- thread-safety of external libraries?
- one thread can corrupt another thread

`FairRoot` design can cater for both (`MP` _via_ `ZeroMQ` or `NanoMQ` message queues)

* Multi-threading & Multi-processing in a C++ world

Modern architectures impose massive challenges on programmability in the context of performance portability.

- massive increase in on-node parallelism
- deep memory hierarchies

Only *portable* parallelization solution for `C++` programmers (today?): *OpenMP* & *MPI*

- hugely successful for years
- widely used and supported
- simple use for simple cases
- very portable
- highly optimized

_Which_ `C++` BTW ?

* C++ timeline

`C++` is a wide and complex language.
Know your `C++` and the (subset of?) `C++` you are *allowed* to write!

- `C++03`?
- `C++11`?
- `C++14`?

.image figs/wg21-timeline.png 300 _

* Parallelism in C++

`C++11` introduced lower level abstractions:

- `std::thread`, `std::mutex`, `std::future`, ...
- fairly limited, more is needed
- `C++` needs stronger support for higher-level parallelism

Several proposals to the Standardization Committee are accepted or under consideration:

- Technical Specification: Concurrency
- Technical Specification: Parallelism
- Other smaller proposals: resumable functions, task regions, executors

* Parallelism in C++

Currently, there is no overarching vision related to higher-level parallelism

- goal is to standardize on a "big story" by 2020
- no need for OpenMP, OpenACC, OpenCL, etc...

But for the moment, `C++` programmers are stuck with `C++11/14`...

* Interlude: memory model & tools

* Memory model

With `C++11`, finally `C++` has a memory model that contemplates a multi-threaded execution of a program.

A thread is a single flow of control within a program

- Every thread can potentially access every object and function in the program
- The interleaving of each thread's instructions is undefined

.image figs/thread-exec-race.png

* Memory model

`C++` guarantees that two threads can update and access *separate* memory locations without interfering with each other.

- For all other situations updates and accesses have to be properly synchronized (_ie:_ atomics, locks, memory fences)
- If updates and accesses to the same location by multiple threads are not properly synchronized, there is a data race (_ie:_ undefined behavior)
- Data races can be made visible by transformations applied by the compiler or by the processor for performance reasons

* Tools to support parallel programming development/debugging

.link http://valgrind.org/docs/manual/drd-manual.html
.link http://valgrind.org/docs/manual/hg-manual.html
.link http://clang.llvm.org/docs/ThreadSanitizer.html

Detect races at runtime:

- needs a test case and workload that triggers a race
- no false positive, but can not detect all possible races

Needs a recompilation (`TSan`), (dramatic) increase of resources requirements at
runtime (CPU, VMem) b/c of code instrumentation and bookkeeping.

* C++ parallel programming: building blocks

* C++11 std::thread

`C++11` (finally!) standardized threads

- `std::thread` is now part of the standard `C++` library
- `std::thread` is an abstraction and maps to local platform threads (`POSIX`, `Windows(TM)`, ...)

* C++11 std::thread

.code code/hello-par.cxx

 $> g++ -o hello-par --std=c++11 -pthread hello-par.cxx
 $> ./hello-par
 ** inside thread 139800850654976!
 
* C++11 std::thread - avoiding errors

.code code/hello-2.cxx

*(1)* Thread function must do *exception*handling*: unhandled exception => program termination

*(2)* Must join with _thread_ *before* handle goes out of scope, otherwise:
program termination.

* Programming style

- Old school: thread functions (what we just saw)
- Middle school: function objects (functors)

.code code/par-functors.cxx

* Programming style

- New school: `C++11` lambda functions (aka anonymous functions)

It's all about trade-offs...

*Lambda*functions:*

- easier and more readable (once your brained has been trained)
- code remains inline
- potentially more *dangerous* (`[&]` captures everything by reference)

*Functions:*

- more efficient: lambdas involve class, function objects
- potentially *safer*: requires explicit variable scoping
- more cumbersome

* Example: matrix multiply

.image figs/matrix-multiply.png

* Sequential version

.image figs/matrix-multiply-seq.png

* Structured (fork-join) parallelism

A common pattern when creating multiple threads 

.image figs/matrix-multiply-fork-join.png

* Parallel solution

.image figs/matrix-multiply-par.png _ 850

* Types of parallelism

Most common types:

- Data
- Task
- Embarrassingly parallel
- Dataflow

* Data parallelism 

.image figs/par-types-data.png

* Task parallelism

.image figs/par-types-task.png

* Embarrassingly parallel

.image figs/par-types-emb-par.png

* Dataflow

.image figs/par-types-dataflow.png

* C++ concurrency features

.image figs/cxx-features.png

* Futures

*Futures* provide a higher level of abstraction

- you start an asynchronous/parallel operation
- you are returned a handle to wait for the result
- thread creation, join and exceptions are handled for you

* std::async + std::future

.image figs/future-async.png

* async operations

.image figs/future-async-2.png

* (no) Conclusions

That's all for today.
Many more to cover, though:

- SIMD
- Thread Building Blocks (`Intel` `TBB`)
- `FPGA`
- (auto-)vectorization
- `OpenCL`/`OpenMP`/`OpenACC`
- HPX
- ...

* To recap

Remember the pizzas?

- know your *resources* (# of chefs, # of kitchens, ovens)
- know the *tasks* (cut, pre-heat, bake) and their *dependencies*, composing the overall goal (pizza)
- know the *latencies* (pre-heating) and the *bottlenecks* (oven) of your main task
- infer/learn the *optimal* resources for the task(s) at hand (some cooks/kitchens are better at making pizzas, other: calzones)

(imperfect) analogy:

- chef: thread of execution
- kitchen: CPU Core, GPU, FPGA, ...
- oven: disk, memory, i/o, ...
 
* Parallel programming: references

.link https://web2.infn.it/esc15
.link http://concur.rspace.googlecode.com/hg/talk/concur.html
.link http://sc13.supercomputing.org/sites/default/files/prog105/prog105.pdf
